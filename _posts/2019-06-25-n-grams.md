---
# layout: splash
title: 'N-Grams for Poetry Generation! (part-2a)'
date: 2019-06-25
permalink: /posts/poem-generation/n-gram/
tags:
  - poem-generation
  - nlp
header: 
  overlay_color: "#123"
  opacity: 1
  overlay_filter: linear-gradient(rgba(20, 20, 20, 0.5), rgba(50, 0, 0, 0.0))
  show_overlay_excerpt: false
  teaser: /images/poem-generation/illustration2.png

---

Here's a poem generated by a 4-gram model!

![ngram1](/images/poem-generation/ngram2.png)


### The Concept of N-Grams!

It is a probabilistic approach to predicting the next word given a sequence of N words preceding it. To demonstrate what I mean clearly, I will use the following sentence:

"Language shapes the way we ___?___"

Goal: To predict the word which will replace the question mark! (call it X)

In order to do this, we make a simplifying assumption, which is, the word X is only dependent on a certain number of words, N words, preceding it.
- For N=4, X would depend on "the way we". 
- For N=2, X would depend only on "we".

- If N=1, it is a Unigram. (X itself, i.e. without any context)
- If N=2, it is a Bigram.
- If N=3, it is a Trigram, and so on!

### How do we use this context of N-words to predict?

We start counting!

I.e. we count the number of times the context has appeared in the dataset where the N-Gram model will learn from. In that context, the word which appeared the most after the context is the word we predict as X!

Example: For a Trigram Model, applied on the above sentence:

We count the total occurrences of "way we" in the database. Then we look for all the words which followed "way we", and we count all of them as well. The word which occurred the most will be predicted as X. Its probability will be given by the formula :

![raw-dict](/images/poem-generation/formula1.png)

That way you could find the probability of any sequence, and use the sequence with the highest probability as your output!

For more mathematical details, you should check out: http://web.stanford.edu/~jurafsky/slp3/3.pdf.

### Computing the N-Grams Model from Scratch!

First, we need to import the dataset and organize it. That makes it convenient for us to work with. Then we need to come up with a clever solution for the data structure we use. Finally, we can start building up the model by filling it with counts to generate our first poem!

Let's get started.

### Importing and organizing the data

Reading the data:


``` python
# The dictionary is called cmudict-0.7b.
import pandas as pd # Easy to read it with pandas
syllables = pd.read_csv("cmudict-0.7b.txt","  ", engine="python")
syllables.head(5)# After renaming the columns.
```

![raw-dict](/images/poem-generation/raw-dict.png)

In the syllable_count column, there seems to be a lot of information, more than we seem to need. We start by converting that column, such that it outputs only the number of syllables present, rather than their types. For more information on what that information represents, please refer the CMU website.

``` python
import pandas as pd
data = pd.read_csv("PoetryFoundationData.csv")
```
Here, I have imported the dataset we created earlier in this series! You should get familiar with the Data it contains!

We now want to convert each Poem into a list of tokens. Tokens may include words, punctuations, and end of sentences. Here, we will make use of the word_tokenize function from the NLTK library to tokenize our Poems!

But, before we do that, we must do some preformating!

Like, we don't want the computer to differentiate between "word" and "Word", i.e. ignore capitalization differences. Hence, we will convert all the words in all the poems to lower case characters.
**Note:** Doing this makes a significant difference.

Also, this "word_tokenize" function will remove all the newlines from our poems, i.e. "\n\r" characters. As Newlines are significant in a poem, we will add a new token called "NEWLINE".

``` python
# Defining the function to make all words lowercased and then
# adding the NEWLINE charachter.
def clean_text(string):
    string  = str(string).lower()
    return str(string).replace("\r\r\n","NEWLINE\n ")
# Applying the function to all Poems in the datafile!
data = data.applymap(clean_text)
```

Now, we can tokenize the text!

``` python
# This module can be simply installed by pip install!
import nltk
# Converting all the poems to a set of tokens!
Poems = data.Poem.map(lambda x: nltk.tokenize.word_tokenize(x))
```

The data is now ready to be used for the N-Grams model!

### Efficiently storing the N-Grams, data structures!

You might wonder, why can't we just iterate over all the Poems, find all the N-grams, and then store them in a list. Honestly, that's what I did initially and the size of the list blew up exponentially to the N-Grams. Moreover, even when I was able to somehow store all the Bigrams, counting each Bigram even with a counter method was too slow. Moreover, after I count each Bigram, I still need quick access to all which start with a certain word so that I can find the one with the highest probablity! Eventually, I realized this way of storing the data is really inefficient. I had a brilliant insight!

I decided to store All the N-grams with nested dictionaries of N-depth!

The first question you might have is, why dictionaries? Dictionaries are great for tasks where you want to look up some information. They have a time complexity of O(1)!

Dictionaries are like Buildings which contains rooms. Each room has the information you are looking for, and each room comes with a key which you can use to instantly find that information!

**How is this helpful for our case?**

"an amazing amazing world" (It's not a typo!)

For Unigrams, the key is the word itself and the value is its count. I.e. the number of occurrences of the word in all the poems! This makes looking up the word's count very efficient!

This would be represented in the data structure as follows:

![raw-dict](/images/poem-generation/illustration1.png)

Next, for Bigrams it becomes slightly complicated. The first word in a Bigram is key. Its value is another dictionary whose keys are all the words which occur after the first word and their values are the counts of the respective Bigrams!

Its bigrams are "an amazing", "amazing amazing", "amazing world". This would be illustrated as follows:

![raw-dict](/images/poem-generation/illustration2.png)

Similarly, for N-grams, we keep on adding dictionaries inside dictionaries, where each subsequent ith dictionary represents N-i gram! The last one being the unigram representing the entire N-grams count!

Now that we have figured out how to store the data structure, let's get started by implementing it!

### Finally, programming the model!

Here, I will simply demonstrate how one would implement the Bi-gram model. As the code for the N-gram model gets a bit too complicated, I will post it on the Git-Hub with complete explanation!

Let's get started!

``` python
# It takes in a dictionary in a case where we have already done some  # training!
# Also takes a poem from which we want to find the Bigrams from!
def BiGram(dict, poem):
    #The total number of bigrams is 1 less than the total tokens!
    for i in range(len(poem)-1):
        #Now we need to extract the word,next_word pair, so:
        word = poem[i]
        next_word  = poem[i+1]
        #We check if the first word already exists in the dict
        if(word in dict):
            #If it does then we check if the next word exists
            #In the dictionary inside the first word!     
            if(next_word in dict[word]):
                #Both words already exist, so we increase the count!
                dict[word][next_word] = dict[word][next_word]+1
            else:
                #Because the next word didnt exist we create an
                # entry for it
                dict[word][next_word] = 1
        else:
            # If the first itself doesn't exist in the dictionary
            # the create entry for both first, second word!
            dict[word]={next_word:1}
    #Finally return the updated dictionary!
    return(dict)
```

Finally, we programmed the data structure for the Bi-gram model! Now it's time to fill it up with the data we cleaned up earlier!

``` python
dicBi = {} # The dictionary for the BiGram model
for poem in Poems: #Feed it all the poems
    dicBi = n.BiGram(dicBi, poem) #Build the model!
```

We have the Bi-Gram representation, along with the counts of all the words. Now we need to be able to predict the next word, given the previous word. This we can generate our first poem!

``` python
def next_word(first_word, dict):
    # We need to find the word with highest count 
    # given the first  word
    
    Max = 0
    next_word = ""
    # Iterating through all the word following the first word!
    for word,count in list(dictionary[first_word].items()):
    # If the word has more count than the maximum, 
    # make it the maximum
        if(count>Max): 
            Max = count
            next_word = word
  
    return(next_word)
```

Now you make the computer generate its very own, unique poems for you!

Here's a poem generated by a Tri-gram model!

![raw-dict](/images/poem-generation/ngram3.png)
