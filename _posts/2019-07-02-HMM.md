---
# layout: splash
title: 'HMM for Poetry Generation! (part-2b)'
date: 2019-07-02
permalink: /posts/poem-generation/hmm/
excerpt: ""
tags:
  - poem-generation
  - nlp
header: 
#   overlay_image: /images/poem-generation/HMMp1.png
  teaser: /images/poem-generation/HMMp1.png
  overlay_color: "#123"
  opacity: 1
  overlay_filter: linear-gradient(rgba(20, 20, 20, 0.5), rgba(50, 0, 0, 0.0))
  show_overlay_excerpt: false

---
A poem generated with hmm!
![hmmp1](/images/poem-generation/HMMp1.png)

---

Previously, we were able to generate a poem using a basic N-gram model. We understood how to implement N-gram models, and that was quite fun! If you missed that article, please check it out here!

## This article's focus

I am going to introduce how we could account for the syllables present in the words, to generate poems with better rhythm and meter!

### Drawbacks of the previous method (simple n-grams)

Before we start, I would like to discuss the elements which the poems generated by a simple N-gram model lacked.

![hmmp1](/images/poem-generation/ngramp1.png)

One of the major problems is after it generates few words/ phrases it starts repeating itself indefinitely. This is because the words generated are purely dependent on words just before them. This prevents us from generating a longer poem. 

Also, the words generated seem to overfit. I.e. the words generated are quite similar to the original poem itself. You might as well call it "copying". Let's see how the improvement mentioned earlier helps with this problem. 
Let's get started!

### The Logic

In order to achieve the aim, we need to implement the following steps.

- Find the number of syllables in a given word.
- Train another N-gram model based on the syllables of words.
- Find a way to combine the two N-gram models! (HMM approach)

### Finding the number of syllables in a given word

After researching quite a bit on this topic, there is no direct algorithm present in the English language which would allow us to find the number of syllables present in a word. The closest approach to finding the number of syllables in a word was mentioned here. Despite all the hand made rules suggested there, it seemed like it wasn't as accurate as we would like it to be.

So here, I take the dictionary approach. Carnegie Mellon University has made a dictionary which consists of words and their syllable break down. They also have a tool, which finds the syllable breakdown for words not present in the dictionary. It's called the CMU Pronouncing Dictionary. We use the above dictionary to tag our poems with the number of syllables present in the words!

We start by downloading this dictionary and importing it in python.

``` python
# The dictionary is called cmudict-0.7b.
import pandas as pd # Easy to read it with pandas
syllables = pd.read_csv("cmudict-0.7b.txt","  ", engine="python")
syllables.head(5)# After renaming the columns.
```

![raw-dict](/images/poem-generation/raw-dict.png)

In the syllable_count column, there seems to be a lot of information, more than we seem to need. We start by converting that column, such that it outputs only the number of syllables present, rather than their types. For more information on what that information represents, please refer the CMU website.

``` python
# A syllable is represented by its stress in the word, i.e.
# primary = 1
# secondary = 2
# no stress = 0
def number_of_syllables(string):
    count = 0
    for ch in string:
        if(ch in "012"):
            count=count+1
    return(count)
#applying the function to the entire column.
syllables.syllable_count = (syllables.syllable_count.map(number_of_syllables))
```

Now we want to convert the Pandas DataFrame to a dictionary.

``` python
# Using a list comprehension, the simplest way.
syllable_dict = dict([(word, syllable)for word,syllable in zip(syllables.words,syllables.syllable_count)])
```

There's still a problem, this dictionary is not complete. It doesn't represent all the words present in all of our poems. More importantly, the words in poems often are deviated from the original word for emphasis. Like, "youuu" instead of "you". We need to find the syllable counts for all such words. So, we generate the list of all the words which are not present in the dictionary.

``` python
# A function which iterates through the words in a poem to check if 
# They are present in the dictionary.
def untagged_words(poem, untagged):
    for token in poem:
        if(not (token in syllable_dict)):
            untagged += [token]
    return(untagged)
# Please refer to poems we generated in the previous article
untagged = []
for poem in poems:
    untagged = tag_syllables(poem, untagged)
untagged = set(untagged)
```

Now that we have all the words which weren't present in the dictionary, we feed them into the CMU Lexicon tool. It generates the possible syllable breakdown of the words. So, we generate a .txt file from the untagged words and input it in the lexicon tool, which generates a new dictionary for us. The new dictionary is read in, processed and converted to a dictionary in the same as above. We then combine the dictionaries. (You might have to apply the same process multiple times!) For complete info please refer to the Jupyter notebook on my GIT page.

Now that we have a complete dictionary of syllable count of words present in our poems, we can start building the N-gram model!

### Train another N-gram model based on the syllables of words.

This is a simple process, as we have already learned how to train N-gram models!

From the dictionary we previously created, we convert all the words present in our poems to their respective syllable counts.

``` python
# The dicitionary we create previously was named syllable_dict
# A function which takes a single poem and converts it 
# into a poem of syllable counts
def poem_of_syllables(poem, syllable_dict):
    syllable_poem = []
    for token in poem:
        if(token in syllable_dict):
            tag = syllable_dict[token]
            syllable_poem = syllable_poem + [tag] 
    return(syllable_poem)
# Converting the poems, to syllable poems by applying the func
syllable_poems = []
for poem in poems:
    temp = syllables_poem(poem,syllable_dict)
    syllable_poems = syllable_poems+[temp]
```

We can now start building the N-grams model! Moreover, all of the code we used last time can be used for this. It would be quite inefficient if were to replicate the same code over and over again. Hence, I modulated the entire code in a python file called Models, which has a class called N-grams which represents it! For more details, on that code please check this out!

Now implementing the N-grams is quite simple!

``` python
import model
FourGram         = model.nGrams(poems, 4)
FourGramSyllable = model.nGrams(syllable_poems , 4)
```

There! We have already trained our 4 gram model for our syllable_poems! Moreover, predicting the next word can be done as simply as:

``` python
FourGram.next_word(["I","LOVE","YOU"])
# It will output, ","
```

There's one change I made to the next_word function compared to what we discussed in the previous article. Instead of predicting the next word based on the pure counts, I rescale the counts into probabilities.

Example, if the dataset was:
- The morning.
- The evening.
- The morning sun.

And we wanted to predict the word after "The", we would predict "morning", as it occurs 2 times, compared to "evening" which only occurs once. But, instead of giving morning a value of 2 and evening a value of 1. We divide by the number of times the word "The" occurs. Hence, the probablity of "morning" being predicted is 2/3, which is 66% and probablity of "evening" is 1/3 which is 33%.

The significance of this will be shown in the next section! With this, now we must find a way to combine this syllable_nGrams model with our n-grams model for predicting the next word!

### Find a way to combine the two N-gram models! (HMM approach)

**The problem:** The Syllable n-grams model will tell us how many syllables the next word must-have. The simple n-grams model will tell us which word the next word should be. It is quite possible that the next word predicted doesn't have the same number of syllables as predicted the syllable n-grams model.

Hence, our model must be able to decide whether it must give more importance to the syllable model, or the words model. I.e. One solution could be if the syllable model predicts the next word to have 2 syllables, then of all the two-syllable words, find the most probable word and predict that. The problem with this solution is, what if in a certain case it's better to ignore the rhythm and focus on the word. Hence, we need a clever way, here we combine their probabilities! This is similar to how a Hidden Markov model functions!

Combing these 3 n-grams models, where 2 of the 3 states are unobservable is what we refer to as Triplet Markov Model!

### The logic
Explaining with an example seems simpler. Consider the following dataset:

- He played tennis.
- Children played chess.
- Children played slow.

Consider the word after "played".

For a simple N-grams model, the word after play would be,

- tennis = 1/3
- chess = 1/3
- slow = 1/3

The words, "He", "chess", "golf" consist of 1 syllable each. 

- 1-syllable word = 2/6 (As chess and golf follow played)
- 2-syllable word = 3/6 (As tennis follows played, played follows children)
- punction = 1/4 (As full-stop follows tennis)

So when we combine the probabilities for all possible words after "played", it would be:

- tennis = 1/3 * 3/6 = 3/18
- chess = 1/3 * 2/6 = 2/18
- golf = 1/3 * 2/6 = 2/18

Hence, in this case, we are more likely to predict the word "tennis" after the word "played"!

### Implementing the above model!

Previously we trained the two models, and the one trained in the last article:

``` python
import model
FourGram         = model.nGrams(poems, 4)
FourGramSyllable = model.nGrams(syllable_poems , 4)
```

We can access the probablity of all the words after a sequence by:

``` python
FourGram.model[word1][word2][word3]
FourGramSyllable.model[syllable1][syllable2][syllable3]
```

As ```FourGram.model```, simply returns the dictionary of the model!
Let's dive into the code directly! It's annotated heavily for understanding

``` python
# The function would need the two n-gram models, along with the preceding words which will be used to predict!
# We also need to be provided with syllable dictionary, so that we can find the number of syllables in the preceding words
def nextSyllableWord(words, wordnGram, syllablenGram, syllable_dic, n):
    # Finding the number of syllables in each word using the dictionary.
    syllables = [syllable_dic[word] for word in words]
    # Getting the n-gram dictionaries from the n-gram objects
    words_nGram = wordnGram.model
    sylla_nGram = syllablenGram.model
    # Converting all the counts into probablities! This was explained earlier.
    wordnGram.next_word(words)
    syllablenGram.next_word(syllables)
    # Please understand the data structure as explained in the first N-grams implementation! 
    #Iterating through it to extract all the words follow the sequence.
    for word in words:
        words_nGram = words_nGram[word]
    for syllable in syllables:
        sylla_nGram = sylla_nGram[syllable]
    #Intializing the outputs.
    Max = 0
    next_word = "Error" # To detect in case no next word is predicted.
    # Looping through all the possible words!   
    for word, probablity_word in list(words_nGram.items()):
        final_prob = 0
        if(word in syllable_dic):#A check To prevent errors
            word_syllable_count = syllable_dic[word]
            final_prob = probablity_word * syllaDic[word_syllable_count] # word_prob * syllable_prob
        if(final_prob>Max): # if that combination is the highest!
            Max = final_prob
            next_word = word
    return(next_word)
```

I really hope that was helpful in implementing, the algorithm!

---
Here are the results generated by the above algorithm!
![raw-dict](/images/poem-generation/HMMp1.png)

---